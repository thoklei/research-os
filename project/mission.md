# Research Mission: Controllable Visual Reasoning Task Generation through Compositional Latent Transformations

## Mission Statement

**Problem Context**

This research addresses the fundamental challenge of understanding and controlling task difficulty in abstract visual reasoning benchmarks. While existing work on the ARC-AGI benchmark has achieved impressive performance through neural cellular automata (Xu & Miikkulainen, 2025), product-of-expert approaches (Franzen et al., 2025), and vision transformers (ViTARC, 2024), these methods operate on tasks with opaque difficulty characteristics and unknown generating processes. Current benchmark datasets provide no mechanism for systematically varying task complexity or understanding which structural features drive reasoning difficulty. The single existing attempt at learned representations for ARC grids through variational autoencoders (arXiv:2311.08083) achieved only 2% accuracy on the official benchmark, highlighting the significant challenge of developing effective latent representations for discrete grid-based reasoning tasks.

**Research Objective and Hypothesis**

Our primary objective is to develop the first controllable generator for ARC-like visual reasoning tasks with mathematically interpretable difficulty parameters by designing a compositional transformation framework in structured latent space. We hypothesize that task difficulty can be precisely controlled through an interpretable parameter: Composition depth of sequential basic transformations. This approach extends beyond current methods by operating in a low-dimensional ($d < 10$) latent space rather than pixel space, enabling systematic analysis of what makes abstract reasoning tasks challenging. Unlike object-centric approaches for ARC that focus on solving tasks (Park et al., 2023), our method targets controlled generation with explicit difficulty parametrization. This specifically addresses the critical gap identified in the literature: no existing work provides continuous, mathematically-defined difficulty control for visual reasoning task generation.

**Methodology**

We propose a compositional latent transformation framework that generates ARC-like tasks through three core components: (1) a custom encoder $\varphi: \mathcal{Z} \rightarrow \text{Grid}$ that maps structured latent representations to discrete 16x16 grids with 10-color palette matching ARC specifications, (2) a library of interpretable sub-functions $f_i$ operating in latent space (geometric transformations: rotate, translate, scale; topological operations: connect, separate; set operations: union, intersection; pattern operations: repeat, symmetrize). The transformation function $F = f_n \circ f_{n-1} \circ \ldots \circ f_1$ composes sub-functions sequentially in latent space, with task instances generated by applying $F$ to sampled input configurations.

**Critical Challenge: Encoder Development**

A fundamental challenge in this approach is obtaining the encoder $\varphi$ that maps latent representations to ARC-like grids. Off-the-shelf vision encoders trained on natural images (ImageNet, CLIP) are unsuitable for abstract grid-based reasoning tasks. The only existing attempt at learned representations for ARC grids (arXiv:2311.08083) achieved merely 2% accuracy, indicating that standard autoencoder approaches fail in this domain. This presents a cold-start problem: we need an encoder to train our compositional generator, but we need data to train the encoder.

Our solution is a two-phase bootstrap approach: First, we develop an **atomic image generator** that produces individual ARC-like grids (not input-output pairs) using simple procedural rules. This generator creates objects through connectivity-biased random growth: starting with a seed pixel, we iteratively add pixels from the 8-neighborhood with preference for positions that maintain object cohesion. The generator produces four object types with varying probabilities: random blobs (40%), filled rectangles (20%), straight lines (20%), and small patterns (20%). Objects are placed on a background with minimum spacing, each assigned a uniform color from the 10-color ARC palette. This atomic generator enables us to create a corpus of 50,000-100,000 synthetic ARC-like images for training the encoder $\varphi$ as an autoencoder. Once trained, this encoder can then be integrated into our compositional transformation framework.

We evaluate our method through two validation strategies: (1) verification that composition depth correlates with human-rated difficulty, and (2) assessment of whether state-of-the-art ARC solvers exhibit performance aligned with our difficulty parameters.

<hypothetical>
Our experiments demonstrate substantial control over task difficulty through "interpretable" parameters. On a benchmark of 1,000 generated tasks, we observe strong correlation (Spearman $\rho = 0.78$, $p < 0.001$) between composition depth and human-rated difficulty scores collected from 50 participants. Tasks with composition depth $n=1$ achieve 94% human solve rate with mean solving time of 21.3 seconds, while tasks with $n=5$ drop to 41% solve rate with mean time of 67.8 seconds.

When evaluating state-of-the-art ARC solvers on our generated benchmark, we observe systematic performance degradation aligned with our difficulty parameters. ViTARC accuracy drops from 87% on $n=1$ tasks to 23% on $n=5$ tasks, while the product-of-experts approach (Franzen et al., 2025) degrades from 72% to 18%. The learned encoder $\varphi$ achieves 96.4% reconstruction accuracy on held-out latent configurations, with interpretable failure modes concentrated on high-composition-depth tasks ($n \geq 6$).

Our diversity analysis shows that the compositional library of 15 base sub-functions generates over 100,000 unique task patterns through combinations up to depth $n=4$, with structural diversity scores (measured via graph edit distance on task structure) exceeding those of 400 public ARC tasks by 2.3x. The method demonstrates robust generation across varying conditions, with consistent human solve rate progressions from 92-95% ($n=1$) to 38-44% ($n=5$) across three independent participant cohorts. Parametric control enables precise difficulty targeting: to achieve 70% target solve rate, our method recommends $n=2.4$ compositions with 3-4 objects, empirically validated to 68-73% solve rate across 200 generated instances.
</hypothetical>

**Contributions and Impact**

This work makes three primary contributions to the visual reasoning and benchmark design communities: (1) We introduce the first controllable generator for ARC-like tasks with continuous, mathematically-interpretable difficulty parameters, enabling systematic investigation of reasoning complexity factors. (2) We demonstrate that composition depth in latent space is a stronger predictor of task difficulty than visual complexity metrics, challenging the assumption that perceptual complexity drives reasoning difficulty. (3) We provide both a novel compositional framework operating in structured latent space and a custom encoder architecture for discrete grid generation, addressing the critical gap of robust autoencoders for abstract reasoning tasks.

**Broader Impact**

The implications of this research extend beyond benchmark generation to enable systematic curriculum design for visual reasoning models, controllable difficulty progression for educational applications, and principled analysis of what constitutes "reasoning" in abstract visual tasks. By providing interpretable control over task difficulty, our approach opens new possibilities for understanding the generalization boundaries of current AI systems and developing more targeted training strategies. This work represents a significant step toward understanding the computational structure of abstract reasoning, with potential applications in cognitive science research, AI safety evaluation through controlled difficulty scaling, and automated generation of reasoning assessments. The compositional framework and difficulty parametrization methodology may generalize to other structured reasoning domains beyond visual grids, including spatial reasoning, planning tasks, and symbolic manipulation problems.
